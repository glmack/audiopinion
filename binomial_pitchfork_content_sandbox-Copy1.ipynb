{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing performance homogeneity of text classification algorithms using McNemar statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow use a McNemar statistic to compare two classification algorithms, a naive bayes classifier and a support vector machine, on a binomial document classification task using a common corpus of music reviews. The objectives of this workflow, which are part of a larger project, are to: \n",
    "- explore and test hypotheses on text classification algorithms on common domains, in terms of relative performance advantages in different scenarios\n",
    "- explore test statistics for comparison of text classification algorithms in varied contexts\n",
    "- explore sci-kit learn nlp libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Import, explore and initially pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    not_rock\n",
       " 1    not_rock\n",
       " 2        rock\n",
       " 3        rock\n",
       " 4    not_rock\n",
       " Name: genre_dichot, dtype: object, array([0, 0, 1, 1, 0]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to postgres database containing pitchfork music reviews\n",
    "conn = psycopg2.connect(\"dbname=pitchfork_reviews\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# query database\n",
    "cur.execute(\"\"\"\n",
    "SELECT genres.genre, content.reviewid, content.content \n",
    "FROM content\n",
    "INNER JOIN genres on content.reviewid = genres.reviewid;\n",
    "\"\"\")\n",
    "\n",
    "# cast to dataframe\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head(5)\n",
    "\n",
    "# drop ~20K rows that contain nulls in genre column\n",
    "df = df.dropna(how='any')\n",
    "\n",
    "# create new column that collapses 8 non-rock genres into a single 'not_rock' category\n",
    "df_2 = df['genre'].replace(['electronic', 'experimental', 'folk/country', 'global', 'jazz',\n",
    "        'metal', 'pop/r&b', 'rap'], 'not_rock')\n",
    "df['genre_dichot'] = df_2\n",
    "df['genre_dichot'].value_counts()\n",
    "\n",
    "# separate datasets into data (review text) and labels (genres), respectively\n",
    "data = df['content'].astype(str)\n",
    "data.head(5)\n",
    "\n",
    "df_genre = pd.DataFrame(df['genre_dichot'])\n",
    "\n",
    "label_strings = df['genre_dichot'].astype(str)\n",
    "label_strings[:5]\n",
    "\n",
    "# converts label strings into numeric values, 0 and 1\n",
    "label_encoder = LabelEncoder()\n",
    "label_nums = label_encoder.fit_transform(label_strings)\n",
    "label_nums.shape\n",
    "np.vstack((label_nums[:10], label_nums[:10]))\n",
    "label_encoder.classes_, len(label_encoder.classes_)\n",
    "\n",
    "# partition data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label_nums, test_size=0.30, random_state=3)\n",
    "\n",
    "# not_rock takes the value 0, and rock takes the value 1\n",
    "label_strings.head(5), label_nums[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Tokenize, vectorize and normalize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts 1000 key words from music reviews into cleaned \"tokens,\" counts tokens, and normalizes using tf-idf\n",
    "# vectorizes words directly from initial data, to tfidf-normalized vectors\n",
    "# builds a dictionary of feature indices\n",
    "# Normalizes word count-based vectors to term frequency inverse document frequency (TF-IDF) to penalize\n",
    "# words that occur in many documents in the corpus and thus are less informative\n",
    "tf_vect = TfidfVectorizer()\n",
    "tf_vect.fit(X_train)\n",
    "X_train_tf = tf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Train models on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Naive Bayes Classifier on training features (X_train_tfidf) and training targets (y_train)\n",
    "# TODO (Lee) - alter the naming convention X_train_tf to reflect tfidf convention\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train linear support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and vectorize per sklearn workflow\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = SGDClassifier(penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)\n",
    "\n",
    "model_svm.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Predict genres of test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorizes test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6096,), 6096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize X_test set, similar to above for train set, EXCEPT calls transform, NOT fit_transform\n",
    "X_test_tfidf = tf_vect.transform(X_test)\n",
    "\n",
    "# inspect shapes\n",
    "X_test.shape, X_test_tfidf.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict genres of test set with naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses model to predict vectorized test set\n",
    "preds_nb = model.predict(X_test_tfidf)\n",
    "probas_nb = model.predict_proba(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 1, 1]), array([0, 0, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the predicted categories, in terms of binomial categories of music genres\n",
    "y_test[:6], preds_nb[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78572796, 0.21427204],\n",
       "       [0.65446581, 0.34553419],\n",
       "       [0.50263802, 0.49736198],\n",
       "       [0.93077674, 0.06922326],\n",
       "       [0.38772961, 0.61227039],\n",
       "       [0.59624457, 0.40375543]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the predictions, in terms of the probability of new reviews ### TODO (Lee) - complete\n",
    "probas_nb[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((y_test[:20], preds_nb[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The L' => 0\n",
      "'Can y' => 0\n",
      "'As Ge' => 0\n",
      "'Few e' => 0\n",
      "'\"We t' => 1\n"
     ]
    }
   ],
   "source": [
    "for doc, category in zip(X_test[:5], preds_nb[:5]):\n",
    "    print('%r => %s' % (doc[:5], category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3282, 1: 2814})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the actual genres of the test set\n",
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 4150, 1: 1946})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the predictions of the naive bayes across the two categories\n",
    "Counter(preds_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - this is not functioning.\n",
    "# Counter(probas_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict genres of test set with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_svm = model_svm.predict(X_test_tfidf)\n",
    "\n",
    "# TODO (Lee) - probas not functioning\n",
    "# probas_svm = model_svm.predict_proba(X_test_tfidf) # TODO (Lee) - issue with probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_svm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probas_svm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((y_test[:5], preds_svm[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3282, 1: 2814})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the actual genres of the test set\n",
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3286, 1: 2810})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the predictions of the naive bayes across the two categories\n",
    "Counter(preds_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - this is not functioning.\n",
    "# Counter(probas_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V. Evaluate and compare model performance on predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate mean accuracy of naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6981627296587927"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the accuracy of the Naive Bayes Classifier in predicting tbe genre of the test set\n",
    "np.mean(preds_nb == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate mean accuracy of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7129265091863517"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the accuracy of the SVM in \n",
    "np.mean(preds_svm == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare performance of naive bayes and svm using McNemar test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 0],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Implement functions to produce data to be passed as input to McNemar\n",
    "# McNemar test expects four values: 1) # obs when both models predict correctly, 2) # obs when both models predict \n",
    "# incorrectly, 3) # obs when nb predicts correctly & svm predicts incorrectly, 4) # obs when nb predicts incorrectly \n",
    "# & svm predicts correctly\n",
    "\n",
    "# creates array of test lables, naive bayes predictions, and svm predictions\n",
    "cont_table = np.vstack((y_test, preds_nb, preds_svm)).T\n",
    "cont_table[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el idx 0 = both correct, el idx 1 = both incorrect\n",
    "# el idx 2 = nbcorrect, svm incorrect, el idx 3 = svm correct, nb incorrect\n",
    "def process_row(row):\n",
    "    \"\"\"for each row in array, returns array representing one of four categories\"\"\"\n",
    "    if row[0] == row[1] and row[0] == row[2]: # \n",
    "        result = [1,0,0,0]\n",
    "    \n",
    "    elif row[0] == row[1]:\n",
    "        result = [0,1,0,0]\n",
    "        \n",
    "    elif row[0] == row[2]:\n",
    "        result = [0,0,1,0]\n",
    "        \n",
    "    else:\n",
    "        result = [0,0,0,1]\n",
    "    \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ndarray(array):\n",
    "    result = sum([process_row(row) for row in array])\n",
    "    return np.array([[result[0], result[2]], [result[1], result[3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls function\n",
    "contingency_table = process_ndarray(cont_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate McNemar test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mcnemar test\n",
    "result = mcnemar(contingency_table, exact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic=615.000, p-value=0.014\n",
      "Different proportions of errors (reject H0)\n"
     ]
    }
   ],
   "source": [
    "# summarizes the finding, with McNemar statistic of 615 and p-value of 0.014\n",
    "print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
    "\n",
    "# defines alpha level\n",
    "alpha = 0.05\n",
    "\n",
    "if result.pvalue > alpha:\n",
    "\tprint('Same proportions of errors (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different proportions of errors (reject H0)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
