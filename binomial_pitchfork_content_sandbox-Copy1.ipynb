{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitchfork Content Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses a McNemar statistic to compare two classification algorithms, a naive bayes classifier and a support vector machine, on a binomial document classification task using a common corpus of music reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.option_context('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_seq_items = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Import, explore and initially pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22690 entries, 0 to 22689\n",
      "Data columns (total 3 columns):\n",
      "genre       20319 non-null object\n",
      "reviewid    22690 non-null int64\n",
      "content     22690 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 531.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20319 entries, 0 to 22688\n",
      "Data columns (total 3 columns):\n",
      "genre       20319 non-null object\n",
      "reviewid    20319 non-null int64\n",
      "content     20319 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 635.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20319 entries, 0 to 22688\n",
      "Data columns (total 1 columns):\n",
      "genre_dichot    20319 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 317.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14223, 6096, 14223, 6096)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create connection to postgres database\n",
    "conn = psycopg2.connect(\"dbname=pitchfork_reviews\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# query database\n",
    "cur.execute(\"\"\"\n",
    "SELECT genres.genre, content.reviewid, content.content \n",
    "FROM content\n",
    "INNER JOIN genres on content.reviewid = genres.reviewid;\n",
    "\"\"\")\n",
    "\n",
    "# cast to dataframe\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "\n",
    "df.head(5), df.info()\n",
    "\n",
    "# drop ~20K rows where nulls in genre columns\n",
    "df = df.dropna(how='any')\n",
    "df.info()\n",
    "\n",
    "# create new column that collapses 8 non-rock genres into a single 'not_rock' category\n",
    "df_2 = df['genre'].replace(['electronic', 'experimental', 'folk/country', 'global', 'jazz',\n",
    "        'metal', 'pop/r&b', 'rap'], 'not_rock')\n",
    "\n",
    "df['genre_dichot'] = df_2\n",
    "\n",
    "df['genre_dichot'].value_counts()\n",
    "\n",
    "# separate datasets into feature values and feature labels, respectively\n",
    "data = df['content'].astype(str)\n",
    "data.head(5)\n",
    "\n",
    "df_genre = pd.DataFrame(df['genre_dichot'])\n",
    "df_genre.info()\n",
    "\n",
    "feature_names = df['genre_dichot'].astype(str)\n",
    "feature_names[:5]\n",
    "\n",
    "# converts label strings into numeric values, 0 and 1\n",
    "label_encoder = LabelEncoder()\n",
    "feature_names_arr = label_encoder.fit_transform(feature_names)\n",
    "feature_names_arr.shape\n",
    "\n",
    "np.vstack((feature_names_arr[:10], feature_names[:10]))\n",
    "\n",
    "# two classes\n",
    "label_encoder.classes_, len(label_encoder.classes_)\n",
    "\n",
    "#### Partition Data\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, feature_names_arr, test_size=0.30, random_state=3)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Tokenize, vectorize and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts words into vocabulary of 1000 cleaned word \"tokens\" represented in a sparse matrix \n",
    "# TODO (Lee) - count vect vs tfidf, ? for every document #TODO (Lee) - count vect vs tfidf\n",
    "# builds a dictionary of feature indices\n",
    "# the index value of a word in the vocabulary is linked to its frequency in the whole training corpus.\n",
    "# converts music reviews into numerical feature vectors, including tokenization, counting and normalization.\n",
    "count_vect = CountVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "X_train_counts.shape\n",
    "\n",
    "# count_vect.vocabulary_\n",
    "\n",
    "#### Normalizes word count-based vectors to term frequency inverse document frequency (TF-IDF)\n",
    "\n",
    "#### Normalizes word count-based vectors to term frequency inverse document frequency (TF-IDF)\n",
    "# computes TF-IDF using `TfidfTransformer`\n",
    "# TF-IDF downscale weights for words that occur in many documents in the corpus and are \n",
    "# therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts) # fits/learns idf vector (global term weights)\n",
    "\n",
    "tf_transformer\n",
    "\n",
    "X_train_tfidf = tf_transformer.transform(X_train_counts) # transform count matrix to a tf-idf representation\n",
    "\n",
    "X_train_tfidf.shape, len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the key step, which vectorizes words directly from initial data, to tfidf-normalized vectors\n",
    "tf_vect = TfidfVectorizer()\n",
    "tf_vect.fit(X_train)\n",
    "X_train_tf = tf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Naive Bayes Classifier to predict genres on test music reviews\n",
    "# train Naive Bayes Classifier on training features (X_train_tfidf) and training targets (y_train)\n",
    "# vectorizes X_test set, similar to above for train set EXCEPT call transform, NOT fit_transform, since fit on train set\n",
    "# since they have already been fit to the training set:\n",
    "# X_test_tfidf = tf_transformer.transform(X_test_counts)\n",
    "# predicted = model.predict(X_test_tfidf)\n",
    "# X_new_counts = count_vect.transform(X_test)\n",
    "# X_new_tfidf = TfidfTransformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (Lee) - note here the X_train_tf convention that I think was from a Miles implementation\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6096,), 6096)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect shapes\n",
    "X_test.shape, X_test_tfidf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test_tfidf)\n",
    "probas = model.predict_proba(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 4150, 1: 1946})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3282, 1: 2814})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command to look up\n",
    "np.vstack((y_test[:20], preds[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, category in zip(X_test[:20], preds[:20]):\n",
    "    print('%r => %s' % (doc[:20], category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics for Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf = Pipeline([\n",
    "#    ('vect', CountVectorizer()),\n",
    "#    ('tfidf', TfidfTransformer()),\n",
    "#    ('clf', MultinomialNB()),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twenty_test = fetch_20newsgroups(subset='test',\n",
    "#    categories=categories, shuffle=True, random_state=42)\n",
    "# docs_test = twenty_test.data\n",
    "# predicted = text_clf.predict(docs_test)\n",
    "# np.mean(predicted == twenty_test.target)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf = Pipeline([\n",
    "#   ('vect', CountVectorizer()),\n",
    "#   ('tfidf', TfidfTransformer()),\n",
    "#   ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "#                         alpha=1e-3, random_state=42,\n",
    "#                         max_iter=5, tol=None)),\n",
    "# ])\n",
    "\n",
    "# text_clf.fit(X_train, y_train)  \n",
    "\n",
    "# predicted_svm = text_clf.predict(X_test)\n",
    "# np.mean(predicted_svm == y_test)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and vectorize per sklearn workflow\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)\n",
    "\n",
    "model_svm.fit(X_train_tf, y_train)\n",
    "\n",
    "preds_svm = model_svm.predict(X_test_tfidf)\n",
    "# probas_svm = model_svm.predict_proba(X_test_tfidf) # TODO (Lee) - issue with probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(preds_svm == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### McNemar test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance,\tClassifier1 Correct,\tClassifier2 Correct\n",
    "1\t\t\tYes\t\t\t\t\t\tNo\n",
    "2\t\t\tNo\t\t\t\t\t\tNo\n",
    "3\t\t\tNo\t\t\t\t\t\tYes\n",
    "4\t\t\tNo\t\t\t\t\t\tNo\n",
    "5\t\t\tYes\t\t\t\t\t\tYes\n",
    "6\t\t\tYes\t\t\t\t\t\tYes\n",
    "7\t\t\tYes\t\t\t\t\t\tYes\n",
    "8\t\t\tNo\t\t\t\t\t\tNo\n",
    "9\t\t\tYes\t\t\t\t\t\tNo\n",
    "10\t\t\tYes\t\t\t\t\t\tYes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table = np.vstack((y_test, preds, preds_svm)).T\n",
    "cont_table[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table[0][0], cont_table[0][1], cont_table[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table[0,0], cont_table[0,1], cont_table[0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- both models predict correctly\n",
    "- both models predict incorrectly \n",
    "- nb predicts correctly & svm predicts incorrectly\n",
    "- nb predicts incorrectly & svm predicts correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el idx 0 = both correct, el idx 1 = both incorrect\n",
    "# el idx 2 = nbcorrect, svm incorrect, el idx 3 = svm correct, nb incorrect\n",
    "def process_row(row):\n",
    "    if row[0] == row[1] and row[0] == row[2]: # \n",
    "        result = [1,0,0,0]\n",
    "    \n",
    "    elif row[0] == row[1]:\n",
    "        result = [0,1,0,0]\n",
    "        \n",
    "    elif row[0] == row[2]:\n",
    "        result = [0,0,1,0]\n",
    "        \n",
    "    else:\n",
    "        result = [0,0,0,1]\n",
    "    \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ndarray(array):\n",
    "    result = sum([process_row(row) for row in array])\n",
    "    return np.array([[result[0], result[2]], [result[1], result[3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = process_ndarray(cont_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mcnemar test\n",
    "result = mcnemar(contingency_table, exact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the finding\n",
    "print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the p-value\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.pvalue > alpha:\n",
    "\tprint('Same proportions of errors (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different proportions of errors (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic = (Yes/No - No/Yes)^2 / (Yes/No + No/Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(57 - 664)**2 / (57 + 664)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where Yes/No is the count of test instances that Classifier1 got correct and Classifier2 got incorrect, and No/Yes is the count of test instances that Classifier1 got incorrect and Classifier2 got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.where(a>0.5,1,0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cont_table:\n",
    "    if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table[cont_table>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_data = {'label': list(cont_table[:, 0]), \n",
    "        'nb_pred': list(cont_table[:,1]), \n",
    "        'svm_pred': list(cont_table[:, 2])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts = pd.DataFrame(cont_table).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_conts\n",
    "df_conts.columns = ['label', 'nb_pred', 'svm_pred']\n",
    "df_conts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts['nb_bool'] = np.where(df_conts['label'] == df_conts['nb_pred'], 'True', 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts['svm_bool'] = np.where(df_conts['label'] == df_conts['svm_pred'], 'True', 'False')\n",
    "df_conts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts.rename(columns={\"nb_pred_bool\": \"nb_bool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts['nb_bool'] = df_conts['nb_bool'].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts['svm_bool'] = df_conts['svm_bool'].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_conts['yes_yes'] = df_conts[['nb_bool','svm_bool']].sum(axis=1) == 2\n",
    "df_conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts.head(5)\n",
    "df_conts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_conts['new'] = df_conts.sum(axis=1).where(df_conts['nb_bool'] == df_conts['svm_bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conts['true_true'] = np.where(df_conts['label'].bool(True) == df_conts['nb_pred'].bool(True), 'True', 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in cont_table:\n",
    "    for j in i:\n",
    "        if j == j == j:\n",
    "            counter +=1\n",
    "print(counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.DataFrame(cont_table)\n",
    "df_label.columns = ['label', 'nb_pred', 'svm_pred']\n",
    "df_label.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['yes_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statistic = (Yes/No - No/Yes)^2 / (Yes/No + No/Yes)\n",
    "\n",
    "statistic = (Yes/No - No/Yes)^2 / (Yes/No + No/Yes)\n",
    "Where Yes/No is the count of test instances that Classifier1 got correct and Classifier2 got incorrect, and No/Yes is the count of test instances that Classifier1 got incorrect and Classifier2 got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes_no(df):\n",
    "    yes_no = []\n",
    "    for i, j in df.iteritems:\n",
    "        if df[0] == df[1] and i[0] != df[2]:\n",
    "            yes_no.append()\n",
    "        return yes_no\n",
    "    for key, value in df.iteritems(): \n",
    "    print(key, value) \n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_conts.iterrows():\n",
    "    yes_no = []\n",
    "    counter = 0\n",
    "    if row['target'] == row['nb_pred']:\n",
    "        counter +=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "    for x in l:\n",
    "        sum += x\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define contingency table\n",
    "table = [[4, 2],\n",
    "\t\t [1, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1\n",
    "2\n",
    "3\n",
    "\t\t\t\t\t\tClassifier2 Correct,\tClassifier2 Incorrect\n",
    "Classifier1 Correct \t??\t\t\t\t\t\t??\n",
    "Classifier1 Incorrect \t?? \t\t\t\t\t\t??\n",
    "In the case of the first cell in the table, we must sum the total number of test instances that Classifier1 got correct and Classifier2 got correct. For example, the first instance that both classifiers predicted correctly was instance number 5. The total number of instances that both classifiers predicted correctly was 4.\n",
    "\n",
    "Another more programmatic way to think about this is to sum each combination of Yes/No in the results table above.\n",
    "\n",
    "\t\t\t\t\t\tClassifier2 Correct,\tClassifier2 Incorrect\n",
    "Classifier1 Correct \tYes/Yes\t\t\t\t\tYes/No\n",
    "Classifier1 Incorrect \tNo/Yes \t\t\t\t\tNo/No\n",
    "1\n",
    "2\n",
    "3\n",
    "\t\t\t\t\t\tClassifier2 Correct,\tClassifier2 Incorrect\n",
    "Classifier1 Correct \tYes/Yes\t\t\t\t\tYes/No\n",
    "Classifier1 Incorrect \tNo/Yes \t\t\t\t\tNo/No\n",
    "The results organized into a contingency table are as follows:\n",
    "\n",
    "\t\t\t\t\t\tClassifier2 Correct,\tClassifier2 Incorrect\n",
    "Classifier1 Correct \t4\t\t\t\t\t\t2\n",
    "Classifier1 Incorrect \t1 \t\t\t\t\t\t3\n",
    "1\n",
    "2\n",
    "3\n",
    "\t\t\t\t\t\tClassifier2 Correct,\tClassifier2 Incorrect\n",
    "Classifier1 Correct \t4\t\t\t\t\t\t2\n",
    "Classifier1 Incorrect \t1 \t\t\t\t\t\t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mcnemar test\n",
    "result = mcnemar(table, exact=True)\n",
    "\n",
    "# summarize the finding\n",
    "print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
    "\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "\n",
    "if result.pvalue > alpha:\n",
    "    print('Same proportions of errors (fail to reject H0)')\n",
    "else:\n",
    "    print('Different proportions of errors (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when calling len() on sparse matrix - TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\n",
    "# inspect shapes - .shape[0] is for sparse matrices, and getnnz gets the count of explicitly-stored values (nonzeros)\n",
    "X_test_tfidf.shape[0], X_test_tfidf.getnnz() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9559c1766fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# (TODO) Lee - this function not functioning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fast path when counter is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "# (TODO) Lee - this function not functioning\n",
    "# Counter(probas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
